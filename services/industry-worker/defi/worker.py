import numpy as npimport pandas as pdimport requestsfrom tqdm import tqdmfrom datetime import timedeltafrom datetime import datetimeimport copyimport warningswarnings.filterwarnings('ignore')token_categories = {    "NFT": ["BLUR", "JPEG", "X2Y2", "BEND", "NFTX", "SUDO", "SAND", "MANA", "ICP", "APE", "FLOW", "OMI", "RMRK", "BAKE", "ERN"],    "Bridge": ["MULTI", "STG", "SYN", "ORC", "CELR", "AXL", "HOP", "REN"],    "Derivatives": ["GMX", "DYDX", "GNS", "APX", "NEST", "PERP", "LVL", "LINA", "NMR", "JRT", "DDX", "MKR"],    "Lending": ["AAVE", "COMP", "XVS", "EUL", "QI", "UWU", "JOE", "TRU", "BETA", "LQTY", "YLD", "YFI", "GFI", "CREAM", "ALPHA"],    "Liquid Staking": ["LDO", "RPL", "ANKR", "SWISE", "FIS", "AAVE", "BNC", "CREAM"],    "Dexes": ["CRV", "UNI", "CAKE", "BAL", "SUSHI", "BSW", "KNC", "1INCH", "JOE", "ZRX", "DODO", "QUICK", "BabyDoge", "CHNG", "LRC", "MDX", "RAY", "BONE", "BAKE", "ORN", "CREAM", "IDEX", "BURGER", "OOE"],    "Yield": ["CVX", "YFI", "ALPACA", "AURA", "SUSHI", "BIFI", "COW", "CTR", "ALPHA", "CHESS", "PENDLE", "BEL", ],    "Ethereum": ["ETH", "UNI", "SHIB", "LDO", "LINK", "LRC", "INST", "RPL", "SWISE", "ANKR", "AAVE", "DYDX", "NMR", "DDX", "TRU", "MKR", "LQTY", "YFI", "FIS", "BONE", "1INCH", "CREAM", "ALPHA", "PENDLE"],    "Polygon": ["MATIC", "AAVE", "LDO", "QUICK", "BAL", "UNI", "BIFI", "CRV", "KLIMA", "INST", "LINK", "ANKR", "AAVE", "GNS", "JRT", "IDEX"],    "Avalanche": ["AVAX", "QI", "JOE", "GMX", "VTX", "PTP", "STG", "INST", "LINK", "ANKR", "AAVE"],    "Binance": ["BNB", "DOGE", "ALPACA", "CAKE", "XVS", "BSW", "PINKSALE", "COW", "THE", "EPS", "DODO", "LINK", "ANKR", "APX", "LVL", "LINA", "MDX", "BAKE", "ORN", "BURGER", "OOE", "CHESS"],    "out_of_categories": ["XRP", "WBTC", "CRO", "QNT", "INJ", "DFI", "KAVA"]}token_list = []for category in token_categories:    for token in token_categories[category]:        token_list.append(token)token_list = list(dict.fromkeys(token_list))print(token_list)def df_column_uniquify(df):    """    Enumerates dublicates of column names    Parameters    ----------    df : pd.Dataframe    Returns    -------    df : pd.Dataframe    """        df_columns = df.columns    new_columns = []    for item in df_columns:        counter = 0        newitem = item        while newitem in new_columns:            counter += 1            newitem = "{}_{}".format(item, counter)        new_columns.append(newitem)    df.columns = new_columns    return dfdef get_available_TVL_from_DefiLlama(token_list):    """    Downloading TVL    Parameters    ----------    token_list : List        List of tokens.    Returns    -------    tvl_series : pd.Series        TVL Series.    """    url = 'https://api.llama.fi/protocols'    request = requests.get(url).json()    llama_data = pd.DataFrame(request)    llama_data = llama_data[['name', 'address', 'symbol', 'chain', 'slug', 'category']]    llama_data = llama_data[llama_data['symbol'].isin(token_list)].reset_index(drop = True)    llama_data['category'] = llama_data['category'].str.replace(' ', '_')        available_llama_slugs = list(llama_data['slug'])    available_llama_symbols = list(llama_data['symbol'])    available_llama_categories = list(llama_data['category'])        tvl_series = []    llama_tvl_endpoint = "https://api.llama.fi/protocol/"    for i in tqdm(range(len(available_llama_slugs))):        ID = available_llama_slugs[i]        token = available_llama_symbols[i]        category = available_llama_categories[i]        request = requests.get(llama_tvl_endpoint + ID).json()        try:            chain_list = list(request['chainTvls'].keys())        except KeyError:            print('Bad Llama ID:', ID)            print('Bad Token:', token)            print('-' * 30)            continue        for chain in chain_list:            tvls = pd.DataFrame(request['chainTvls'][chain]['tvl'])[ : -1]            tvls['date'] = pd.to_datetime(tvls['date'], unit = 's')            tvls['date'] = tvls['date'].apply(lambda x: x.round(freq = 'D'))            symbol = token + '_' + category + '_' + chain + '_' + ID            protocol_chain_tvl = pd.Series(tvls['totalLiquidityUSD'].values, index = tvls['date'].values, name = symbol)            tvl_series.append(protocol_chain_tvl)    return tvl_seriesdef cut_and_fill_series(tvl_series, start_date, end_date, max_space):    """    Data Preparation:        Cutting on dates and fill spaces    Parameters    ----------    tvl_series : pd.Series        TVL Series.    start_date : Str of date        Format: '2021-08-31'.    end_date : Str of date        Format: '2021-08-31'.    max_space : int        Max number of spaces.    Returns    -------    tvl_dataframes : List[pd.Dataframe]        List of TVL.    """    tvl_dataframes = []        for i in range(len(tvl_series)):            series = copy.deepcopy(tvl_series[i])        token = series.name        start_timestamp = pd.to_datetime(start_date)        end_timestamp = pd.to_datetime(end_date)                series = series[series.index >= start_timestamp]        for i in range(len(series) - 1):            if (series.index[i + 1] - series.index[i]).days > max_space:                start_timestamp = series.index[i + 1]        series = series[series.index >= start_timestamp]        series = series[series.index <= end_timestamp]        if len(series) == 0:            continue        i = 0        while len(series) - 1 != (series.index[-1] - series.index[0]).days:            if (series.index[i + 1] - series.index[i]).days != 1:                series[series.index[i] + timedelta(days = 1)] = series[i]                series = series.sort_index()            i += 1        tvl_dataframes.append(series)    return tvl_dataframesdef print_spaces_in_tvls(tvl_series, max_space):    """    TVL spaces checker.    Parameters    ----------    tvl_series : pd.Series        TVL Series.    max_space : int        max number of spaces.    Returns    -------    None.    """    for j in range(len(tvl_series)):        series = copy.deepcopy(tvl_series[j])        flag = 0        token = series.name    #     df = df.astype({token: float})        compare = (series - series.shift(1)).dropna()        indexes = pd.Series(series.index)        compare_indexes = (indexes - indexes.shift(1)).dropna()        compare_indexes = compare_indexes.apply(lambda x: x.days)        compare.index = compare_indexes        for i in range(len(compare) - 1):            if compare.index[i] > 3:                print('(', j, ') ', token, ': Index - ', i, ', Number of Missing Days - ', \                      int(compare.index[i]) - 1, sep = '')                flag = 1        if flag == 1:            print('-' * 80)def series_to_dataframe(tvl_series, start_date, end_date):    """    Combines Series into Dataframe    Parameters    ----------    tvl_series : List[pd.Series]        List of TVL Series.    start_date : Str of date        Format: '2021-08-31'.    end_date : Str of date        Format: '2021-08-31'.            Returns    -------    llama_tvls : pd.Dataframe        TVL Dataframe.    """    llama_tvls = pd.DataFrame()    start_date = pd.to_datetime(start_date)    end_date = pd.to_datetime(end_date)    number_of_days = (end_date - start_date).days + 1    flag = 0    for i in tvl_series:        series = copy.deepcopy(i)        token = series.name        if len(series) == number_of_days:            if flag == 0:                llama_tvls.index = series.index                flag = 1        else:            end_date = series.index[0]            date_list = []            for i in range((end_date - start_date).days):                date_list.append(start_date + timedelta(days = i))            nan_series = pd.Series(np.nan, index = date_list, name = token)            series = pd.concat([nan_series, series])        tvl_series = series        llama_tvls = pd.concat([llama_tvls, tvl_series], axis = 1)    llama_tvls = df_column_uniquify(llama_tvls)    llama_tvls = llama_tvls.reindex(sorted(llama_tvls.columns), axis = 1)    return llama_tvlsdef get_TVL_data(start_date, end_date = None, max_space = 3, print_spaces = False, token_list = token_list):    """    Main downloader    Usage example:        tvl_data = get_TVL_data(token_list = token_list, start_date = '2021-08-31', end_date = '2023-02-28')    Parameters    ----------    token_list : List        List of tokens.    start_date : Str of date        Format: '2021-08-31'.    end_date : Str of date        Format: '2021-08-31'.    max_space : int        Max number of spaces.    print_spaces : Bool, optional        Flag to print spaces. The default is False.    Returns    -------    tvl_dataframe : pd.Dataframe        TVL dataframe.    """    if end_date == None:        end_date = datetime.today().strftime('%Y-%m-%d')    tvl_series = get_available_TVL_from_DefiLlama(token_list)    filled_tvl_series = cut_and_fill_series(tvl_series, start_date, end_date, max_space)    if print_spaces == True:        print_spaces_in_tvls(tvl_series, max_space = 3)    tvl_dataframe = series_to_dataframe(filled_tvl_series, start_date, end_date)    return tvl_dataframe#TO DO fix this shit so it doesn't need all data but only start datedef update_tvl_data(old_tvl_data):    """    Updates TVL Data    Parameters    ----------    old_tvl_data : pd.Dataframe        Old data from postgres.    Returns    -------    pd.Dataframe        Updated TVL data.    """    start_date = old_tvl_data.index[-1] + timedelta(days = 1)    today = pd.to_datetime(datetime.today().strftime('%Y-%m-%d'))    if old_tvl_data.index[-1] == today:        print('There is nothing to update!')        return old_tvl_data    new_tvl_data = get_TVL_data(token_list = token_list, start_date = start_date, end_date = None, max_space = 3)    full_tvl_data = pd.concat([old_tvl_data, new_tvl_data])    return full_tvl_data