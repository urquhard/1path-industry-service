import numpy as npimport pandas as pdimport requestsimport jsonfrom tqdm import tqdmfrom datetime import timedeltafrom datetime import datetimeimport timeimport copyimport warningswarnings.filterwarnings('ignore')token_categories = {    "NFT": ["BLUR", "JPEG", "X2Y2", "BEND", "NFTX", "SUDO", "SAND", "MANA", "APE", "FLOW", "OMI", "BAKE", "ERN"],    "Bridge": ["MULTI", "STG", "SYN", "ORC", "CELR", "AXL", "HOP", "REN"],    "Derivatives": ["GMX", "DYDX", "GNS", "APX", "NEST", "PERP", "LVL", "LINA", "NMR", "JRT", "DDX", "MKR"],    "Lending": ["AAVE", "COMP", "XVS", "EUL", "QI", "UWU", "JOE", "TRU", "BETA", "LQTY", "YLD", "YFI", "GFI", "CREAM", "ALPHA"],    "Liquid Staking": ["LDO", "RPL", "ANKR", "SWISE", "FIS", "AAVE", "BNC", "CREAM"],    "Dexes": ["CRV", "UNI", "CAKE", "BAL", "SUSHI", "BSW", "KNC", "1INCH", "JOE", "ZRX", "DODO", "QUICK", "BabyDoge", "CHNG", "LRC", "MDX", "RAY", "BONE", "BAKE", "ORN", "CREAM", "IDEX", "BURGER", "OOE"],    "Yield": ["CVX", "YFI", "ALPACA", "AURA", "SUSHI", "BIFI", "COW", "CTR", "ALPHA", "CHESS", "PENDLE", "BEL"],    "Ethereum": ["ETH", "UNI", "SHIB", "LDO", "LINK", "LRC", "INST", "RPL", "SWISE", "ANKR", "AAVE", "DYDX", "NMR", "DDX", "TRU", "MKR", "LQTY", "YFI", "FIS", "BONE", "1INCH", "CREAM", "ALPHA", "PENDLE"],    "Polygon": ["MATIC", "AAVE", "LDO", "QUICK", "BAL", "UNI", "BIFI", "CRV", "KLIMA", "INST", "LINK", "ANKR", "AAVE", "GNS", "JRT", "IDEX"],    "Avalanche": ["AVAX", "QI", "JOE", "GMX", "VTX", "PTP", "STG", "INST", "LINK", "ANKR", "AAVE"],    "Binance": ["BNB", "DOGE", "ALPACA", "CAKE", "XVS", "BSW", "PINKSALE", "COW", "THE", "EPS", "DODO", "LINK", "ANKR", "APX", "LVL", "LINA", "MDX", "BAKE", "ORN", "BURGER", "OOE", "CHESS"],    "out_of_categories": ["XRP", "WBTC", "CRO", "QNT", "INJ", "DFI", "KAVA"]}# Функция качает из DefiLlama информацию о протоколах (адреса, названия, слаги) и выкидывает ненужные протоколыdef get_all_llama_slugs(token_list):        url = 'https://api.llama.fi/protocols'    request = requests.get(url).json()    llama_data = pd.DataFrame(request)    llama_data = llama_data[['name', 'address', 'symbol', 'chain', 'slug', 'category']]    llama_data = llama_data[llama_data['symbol'].isin(token_list)].reset_index(drop = True)    llama_data = llama_data.sort_values('symbol')    llama_data = llama_data[(llama_data['symbol'] != 'AURA') | (llama_data['slug'] == 'aura')]    llama_data = llama_data[(llama_data['symbol'] != 'AXL') | (llama_data['slug'] == 'axelar')]    llama_data = llama_data[(llama_data['symbol'] != 'BONE') | (llama_data['slug'] == 'shibaswap')]    llama_data = llama_data[(llama_data['symbol'] != 'BURGER') | (llama_data['slug'] == 'burgerswap')]    llama_data = llama_data[(llama_data['symbol'] != 'CAKE') | (llama_data['slug'] != 'cakedao')]    llama_data = llama_data[(llama_data['symbol'] != 'COW') | (llama_data['slug'] == 'coinwind')]    llama_data = llama_data[(llama_data['symbol'] != 'CREAM') | (llama_data['slug'] != 'icecream-finance')]    llama_data = llama_data[(llama_data['symbol'] != 'GFI') | (llama_data['slug'] == 'goldfinch')]    llama_data = llama_data[(llama_data['symbol'] != 'HOP') | (llama_data['slug'] == 'hop-protocol')]    llama_data = llama_data[(llama_data['symbol'] != 'NEST') | (llama_data['slug'] == 'nestfi')]    llama_data = llama_data[(llama_data['symbol'] != 'QI') | (llama_data['slug'] == 'benqi-lending')]    llama_data = llama_data[(llama_data['symbol'] != 'SYN') | (llama_data['slug'] == 'synapse')]    llama_data = llama_data.reset_index(drop = True)    return llama_data# Функция обрезает ряды из TVL по введенным датам и заполняет пробелы в данныхdef cut_and_fill_series(series, start_date, end_date, max_space = 3):    start_timestamp = pd.to_datetime(start_date)    end_timestamp = pd.to_datetime(end_date)    series = series[series.index >= start_timestamp]    for i in range(len(series) - 1):        if (series.index[i + 1] - series.index[i]).days > max_space:            start_timestamp = series.index[i + 1]    series = series[series.index >= start_timestamp]    series = series[series.index <= end_timestamp]    if len(series) == 0:        return series    i = 0    while len(series) - 1 != (series.index[-1] - series.index[0]).days:        if (series.index[i + 1] - series.index[i]).days != 1:            series[series.index[i] + timedelta(days = 1)] = series[i]            series = series.sort_index()        i += 1    return seriesdef fill_series_with_nans(series, start_time, end_time):    start_time = pd.to_datetime(start_time)    end_time = pd.to_datetime(end_time)    for i in range((end_time - start_time  + timedelta(days = 1)).days):        date = (start_time + timedelta(days = i))  #.strftime('%Y-%m-%d')        try:            series.loc[date]        except KeyError:            series.loc[date] = np.nan     filled_series = series.sort_index()    return filled_series# Функция выкачивает TVL из DefiLlama и записывает их в словарикdef get_available_TVL_from_DefiLlama(llama_data, start_date, end_date):        chain_tokens_series = pd.Series(['Ethereum', 'Binance', 'Polygon', 'Avalanche', 'Kava'], \                                index = ['ETH', 'BNB', 'MATIC', 'AVAX', 'KAVA'])    count_protocol = 0    tvl_dict = {}    print('Loading TVL of all protocols')    for i in tqdm(range(len(llama_data))):        ID = llama_data['slug'][i]        token = llama_data['symbol'][i]        if token in chain_tokens_series:            continue                key_token = llama_data['symbol'][i]        if (i != len(llama_data['symbol']) - 1) and (i != 0):             if (llama_data['symbol'][i] == llama_data['symbol'][i + 1]):                count_protocol += 1                key_token = llama_data['symbol'][i] + '_' + str(count_protocol)            elif (llama_data['symbol'][i] == llama_data['symbol'][i - 1]):                count_protocol += 1                key_token = llama_data['symbol'][i] + '_' + str(count_protocol)                count_protocol = 0            else:                count_protocol = 0                llama_tvl_endpoint = "https://api.llama.fi/protocol/"        request = requests.get(llama_tvl_endpoint + ID).json()        try:            chain_list = list(request['chainTvls'].keys())        except KeyError:            print('Bad Llama ID:', ID)            print('Bad Token:', token)            print('-' * 30)            continue        mini_dict = {}        for chain in chain_list:            tvls = pd.DataFrame(request['chainTvls'][chain]['tvl'])[ : -1]            tvls['date'] = pd.to_datetime(tvls['date'], unit = 's')            tvls['date'] = tvls['date'].apply(lambda x: x.round(freq = 'D'))            symbol = token            protocol_chain_tvl = pd.Series(tvls['totalLiquidityUSD'].values, index = tvls['date'].values, name = symbol)                        cutted_protocol_chain_tvl = cut_and_fill_series(protocol_chain_tvl, start_date, end_date)                                    filled_protocol_chain_tvl = fill_series_with_nans(cutted_protocol_chain_tvl, start_date, end_date)                      filled_protocol_chain_tvl.index = filled_protocol_chain_tvl.index.strftime('%Y-%m-%d')            mini_dict[chain] = filled_protocol_chain_tvl.to_dict()                    tvl_dict[key_token] = mini_dict                    print()    print('Loading TVL of necessary chains')    for chain_token in tqdm(chain_tokens_series.index):        mini_dict = {}        llama_chain_endpoint = 'https://api.llama.fi/v2/historicalChainTvl/' + chain_tokens_series[chain_token]        request = requests.get(llama_chain_endpoint).json()        chain_tvls = pd.DataFrame(request)        chain_tvls['date'] = pd.to_datetime(chain_tvls['date'], unit = 's')        chain_tvls['date'] = chain_tvls['date'].apply(lambda x: x.round(freq = 'D'))        chain_tvl_series = pd.Series(chain_tvls['tvl'].values, index = chain_tvls['date'])        cutted_chain_tvls = cut_and_fill_series(chain_tvl_series, start_date, end_date)        filled_chain_tvls = fill_series_with_nans(cutted_chain_tvls, start_date, end_date)        filled_chain_tvls.index = filled_chain_tvls.index.strftime('%Y-%m-%d')        mini_dict[chain_tokens_series[chain_token]] = filled_chain_tvls.to_dict()        tvl_dict[chain_token] = mini_dict            return tvl_dict# Функция достает из CoinGecko все id нужных нам протоколов и убирает ненужныеdef get_all_gecko_ids(list_of_tokens):        gecko_endpoint = 'https://api.coingecko.com/api/v3/coins/list'    request = requests.get(gecko_endpoint).json()    cg_info = pd.DataFrame(request)    cg_info = cg_info.sort_values('symbol').reset_index(drop = True)    cg_info['symbol'] = cg_info['symbol'].str.upper()    cg_info['symbol'].replace('BABYDOGE', 'BabyDoge', inplace = True)    cg_info = cg_info[cg_info['symbol'].isin(list_of_tokens)].reset_index(drop = True)    cg_info = cg_info[(cg_info['symbol'] != 'ALPHA') | (cg_info['id'] == 'alpha-finance')]    cg_info = cg_info[(cg_info['symbol'] != 'APE') | (cg_info['id'] == 'apecoin')]    cg_info = cg_info[(cg_info['symbol'] != 'APX') | (cg_info['id'] == 'apollox-2')]    cg_info = cg_info[(cg_info['symbol'] != 'AURA') | (cg_info['id'] == 'aura-finance')]    cg_info = cg_info[(cg_info['symbol'] != 'AVAX') | (cg_info['id'] == 'avalanche-2')]    cg_info = cg_info[(cg_info['symbol'] != 'AXL') | (cg_info['id'] == 'axelar')]    cg_info = cg_info[(cg_info['symbol'] != 'BabyDoge') | (cg_info['id'] == 'baby-doge-coin')]    cg_info = cg_info[(cg_info['symbol'] != 'BEND') | (cg_info['id'] == 'benddao')]    cg_info = cg_info[(cg_info['symbol'] != 'BETA') | (cg_info['id'] == 'beta-finance')]    cg_info = cg_info[(cg_info['symbol'] != 'BIFI') | (cg_info['id'] == 'beefy-finance')]    cg_info = cg_info[(cg_info['symbol'] != 'BNB') | (cg_info['id'] == 'binancecoin')]    cg_info = cg_info[(cg_info['symbol'] != 'BONE') | (cg_info['id'] == 'bone-shibaswap')]    cg_info = cg_info[(cg_info['symbol'] != 'CHESS') | (cg_info['id'] == 'tranchess')]    cg_info = cg_info[(cg_info['symbol'] != 'COMP') | (cg_info['id'] == 'compound-governance-token')]    cg_info = cg_info[(cg_info['symbol'] != 'COW') | (cg_info['id'] == 'coinwind')]    cg_info = cg_info[(cg_info['symbol'] != 'CREAM') | (cg_info['id'] == 'cream-2')]    cg_info = cg_info[(cg_info['symbol'] != 'CTR') | (cg_info['id'] == 'concentrator')]    cg_info = cg_info[(cg_info['symbol'] != 'DFI') | (cg_info['id'] == 'defichain')]    cg_info = cg_info[(cg_info['symbol'] != 'DOGE') | (cg_info['id'] == 'dogecoin')]    cg_info = cg_info[(cg_info['symbol'] != 'DYDX') | (cg_info['id'] == 'dydx')]    cg_info = cg_info[(cg_info['symbol'] != 'ERN') | (cg_info['id'] == 'ethernity-chain')]    cg_info = cg_info[(cg_info['symbol'] != 'ETH') | (cg_info['id'] == 'ethereum')]    cg_info = cg_info[(cg_info['symbol'] != 'FLOW') | (cg_info['id'] == 'flow')]    cg_info = cg_info[(cg_info['symbol'] != 'GFI') | (cg_info['id'] == 'goldfinch')]    cg_info = cg_info[(cg_info['symbol'] != 'HOP') | (cg_info['id'] == 'hop-protocol')]    cg_info = cg_info[(cg_info['symbol'] != 'LDO') | (cg_info['id'] == 'lido-dao')]    cg_info = cg_info[(cg_info['symbol'] != 'LINA') | (cg_info['id'] == 'linear')]    cg_info = cg_info[(cg_info['symbol'] != 'LVL') | (cg_info['id'] == 'level')]    cg_info = cg_info[(cg_info['symbol'] != 'MANA') | (cg_info['id'] == 'decentraland')]    cg_info = cg_info[(cg_info['symbol'] != 'MDX') | (cg_info['id'] == 'mdex')]    cg_info = cg_info[(cg_info['symbol'] != 'ORC') | (cg_info['id'] == 'orbit-chain')]    cg_info = cg_info[(cg_info['symbol'] != 'QI') | (cg_info['id'] == 'benqi')]    cg_info = cg_info[(cg_info['symbol'] != 'QUICK') | (cg_info['id'] == 'quickswap')]    # у матросов есть вопросы    cg_info = cg_info[(cg_info['symbol'] != 'SAND') | (cg_info['id'] == 'the-sandbox')]    cg_info = cg_info[(cg_info['symbol'] != 'SHIB') | (cg_info['id'] == 'shiba-inu')]    cg_info = cg_info[(cg_info['symbol'] != 'THE') | (cg_info['id'] == 'thena')]    cg_info = cg_info[(cg_info['symbol'] != 'TRU') | (cg_info['id'] == 'truefi')]    cg_info = cg_info[(cg_info['symbol'] != 'UNI') | (cg_info['id'] == 'uniswap')]    cg_info = cg_info[(cg_info['symbol'] != 'VTX') | (cg_info['id'] == 'vector-finance')]    cg_info = cg_info[(cg_info['symbol'] != 'XRP') | (cg_info['id'] == 'ripple')]    cg_info = cg_info[(cg_info['symbol'] != 'APE') | (cg_info['id'] == 'apecoin')]    cg_info = cg_info.reset_index(drop = True)        return cg_info# Функция достает PMCV протокола по gecko_iddef get_token_data_from_coingecko(gecko_id):        url = 'https://api.coingecko.com/api/v3/coins/' + gecko_id + '/market_chart?vs_currency=usd&days=max&interval=daily'    request = requests.get(url).json()    token_info_df = pd.DataFrame.from_dict(request)    try:        split_prices = pd.DataFrame(token_info_df['prices'].tolist(), columns = ['timestamp', 'prices'])    except KeyError:        print(request)        return    split_market_caps = pd.DataFrame(token_info_df['market_caps'].tolist(), columns = ['time1', 'market_caps'])    split_volumes = pd.DataFrame(token_info_df['total_volumes'].tolist(), columns = ['time2', 'total_volumes'])    token_info_df = pd.concat([split_prices, split_market_caps, split_volumes], axis = 1)    token_info_df.index = pd.to_datetime(token_info_df['timestamp'] / 1000, unit = 's')    token_info_df = token_info_df.drop(['time1', 'time2', 'timestamp'], axis = 1)    return token_info_df# Функция обрезает датафреймы из PMCV по введенным датам и заполняет пробелы в данныхdef cut_and_fill_dataframe(dataframe, start_date, end_date, max_space = 3):      df = copy.deepcopy(dataframe)[ : -1]    token = df.index.name    start_timestamp = pd.to_datetime(start_date)    end_timestamp = pd.to_datetime(end_date)    df = df[df.index >= start_timestamp]    for i in range(len(df) - 1):        if (df.index[i + 1] - df.index[i]).days > max_space:            start_timestamp = df.index[i + 1]    df = df[df.index >= start_timestamp]    df = df[df.index <= end_timestamp]    if len(df) == 0:        return df    i = 0    while len(df) - 1 != (df.iloc[-1].name - df.iloc[0].name).days:        if (df.iloc[i + 1].name - df.iloc[i].name).days != 1:            new_date = pd.Series(df.iloc[i], name = df.iloc[i].name + timedelta(days = 1))            df = pd.concat([df.T, new_date], axis = 1).T            df = df.sort_index()        i += 1    df.index.name = token        return dfdef fill_dataframe_with_nans(dataframe, start_time, end_time):     start_time = pd.to_datetime(start_time)    end_time = pd.to_datetime(end_time)    for i in range((end_time - start_time  + timedelta(days = 1)).days):        date = (start_time + timedelta(days = i))   #.strftime('%Y-%m-%d')        try:            dataframe.loc[date]        except KeyError:            dataframe.loc[date] = np.nan     filled_dataframe = dataframe.sort_index()        return filled_dataframe# Функция достает все данные из CoinGecko и сохраняет их в словарикеdef download_full_gecko_data(gecko_info, start_date, end_date):        list_of_tokens = list(gecko_info['symbol'])    list_of_gecko_ids = list(gecko_info['id'])    gecko_dict = {}        print()    print('Loading PMCV of all tokens')    for i in tqdm(range(len(list_of_gecko_ids))):        gecko_id = list_of_gecko_ids[i]        token_name = list_of_tokens[i]        try:            if (i % 14 == 0) and (i != 0):    # TIMEOUT                time.sleep(120)            token_data = get_token_data_from_coingecko(gecko_id)        except ValueError:            print(token_name)            continue        token_data = token_data.rename_axis(token_name)                cutted_token_data = cut_and_fill_dataframe(dataframe = token_data, start_date = start_date, end_date = end_date)                filled_token_data = fill_dataframe_with_nans(dataframe = cutted_token_data, start_time = start_date, end_time = end_date)                filled_token_data = filled_token_data.reset_index()        filled_token_data[token_name] = filled_token_data[token_name].apply(lambda x: x.strftime('%Y-%m-%d'))        filled_token_data = filled_token_data.set_index(token_name)                        prices_minidict = filled_token_data['prices'].to_dict()        market_caps_minidict = filled_token_data['market_caps'].to_dict()        volumes_minidict = filled_token_data['total_volumes'].to_dict()        minidict = {'prices': prices_minidict, 'market_caps': market_caps_minidict, 'volumes': volumes_minidict}        gecko_dict[token_name] = minidict            return gecko_dict# Функция, которая сортирует словарь по ключамdef sort_dict_by_keys(dictionary):    keys = list(dictionary.keys())    keys.sort()    sorted_dict = {i: dictionary[i] for i in keys}    return sorted_dict# Функция, которая преобразует словарь в длинный датафреймdef dict_to_dataframe(dictionary, token_addresses):        df_columns = ['date', 'symbol', 'gecko_id', 'llama_id', 'category', 'chain', 'address', 'price', 'market_cap', 'volume', 'TVL']    long_df = pd.DataFrame(columns = df_columns)    chain_tokens = ['ETH', 'BNB', 'MATIC', 'AVAX', 'KAVA']        for token_key in dictionary.keys():        try:            for chain_key in dictionary[token_key]['TVL'].keys():                average_not_small_df = pd.DataFrame(columns = df_columns)                average_not_small_df['date'] = list(dictionary[token_key]['prices'].keys())                average_not_small_df['symbol'] = dictionary[token_key]['symbol']                average_not_small_df['gecko_id'] = dictionary[token_key]['gecko_id']                average_not_small_df['llama_id'] = dictionary[token_key]['llama_id']                average_not_small_df['category'] = dictionary[token_key]['category']                average_not_small_df['chain'] = chain_key                symbol = dictionary[token_key]['symbol']                chain_string_split = copy.deepcopy(chain_key).split('-')                address = None                for address_key in token_addresses[symbol]:                    for chain_substring in chain_string_split:                        lower_chain_substring = chain_substring.lower()                        if lower_chain_substring in address_key:                            address = token_addresses[symbol][address_key]                average_not_small_df['address'] = address                average_not_small_df['price'] = dictionary[token_key]['prices'].values()                average_not_small_df['market_cap'] = dictionary[token_key]['market_caps'].values()                average_not_small_df['volume'] = dictionary[token_key]['volumes'].values()                average_not_small_df['TVL'] = dictionary[token_key]['TVL'][chain_key].values()                long_df = pd.concat([long_df, average_not_small_df])                        except AttributeError:            average_not_small_df = pd.DataFrame(columns = df_columns)            average_not_small_df['date'] = list(dictionary[token_key]['prices'].keys())            average_not_small_df['symbol'] = dictionary[token_key]['symbol']            average_not_small_df['gecko_id'] = dictionary[token_key]['gecko_id']            average_not_small_df['llama_id'] = dictionary[token_key]['llama_id']            average_not_small_df['category'] = dictionary[token_key]['category']            average_not_small_df['chain'] = None            average_not_small_df['price'] = dictionary[token_key]['prices'].values()            average_not_small_df['market_cap'] = dictionary[token_key]['market_caps'].values()            average_not_small_df['volume'] = dictionary[token_key]['volumes'].values()            average_not_small_df['TVL'] = None            long_df = pd.concat([long_df, average_not_small_df])                long_df = long_df.reset_index(drop = True)            return long_df     # Главная функцияdef get_full_data(token_industries, start_date, end_date):        # Достаем все адреса токенов    f = open('token_addresses.json')    token_addresses = json.load(f)    chain_tokens = ['ETH', 'BNB', 'MATIC', 'AVAX', 'KAVA']            # Составляем список токенов    list_of_tokens = []    for category in token_categories:        for token in token_categories[category]:            list_of_tokens.append(token)    list_of_tokens = list(dict.fromkeys(list_of_tokens))            # Достаем TVL    llama_data = get_all_llama_slugs(token_list = list_of_tokens)    TVL_dict = get_available_TVL_from_DefiLlama(llama_data = llama_data, start_date = start_date, end_date = end_date)            # Достаем PMCV#     time.sleep(120)    gecko_data = get_all_gecko_ids(list_of_tokens = list_of_tokens)    PMCV_dict = download_full_gecko_data(gecko_info = gecko_data, start_date = start_date, end_date = end_date)            # Составляем словарь    BIG_DICT = {}    count_protocol = 0    # Добавляем протоколы, которые есть в DefiLlama    for i in range(len(llama_data)):        key_token = llama_data['symbol'][i]        if (i != len(llama_data) - 1) and (i != 0):             if (llama_data['symbol'][i] == llama_data['symbol'][i + 1]):                count_protocol += 1                key_token = llama_data['symbol'][i] + '_' + str(count_protocol)            elif (llama_data['symbol'][i] == llama_data['symbol'][i - 1]):                count_protocol += 1                key_token = llama_data['symbol'][i] + '_' + str(count_protocol)                count_protocol = 0            else:                count_protocol = 0        symbol = llama_data['symbol'][i]        name = llama_data['name'][i]                if symbol in chain_tokens:            wrapped_symbol = 'W' + symbol            address = token_addresses[wrapped_symbol]        else:            address = token_addresses[symbol]                category = llama_data['category'][i]        llama_id = llama_data['slug'][i]        gecko_id = gecko_data[gecko_data['symbol'] == symbol]['id'].values[0]                mini_dict = {'symbol': symbol, 'name': name, 'address': address, 'category': category, 'llama_id': llama_id, 'gecko_id': gecko_id}        BIG_DICT[key_token] = mini_dict                BIG_DICT[key_token]['prices'] = PMCV_dict[symbol]['prices']        BIG_DICT[key_token]['market_caps'] = PMCV_dict[symbol]['market_caps']        BIG_DICT[key_token]['volumes'] = PMCV_dict[symbol]['volumes']        BIG_DICT[key_token]['TVL'] = TVL_dict[key_token]                    # Добавляем протоколы, которых нет в DefiLlama    used_tokens = list(llama_data['symbol'].unique())    missing_tokens = list(set(list_of_tokens) - set(used_tokens))    missing_tokens.sort()        for symbol in missing_tokens:                key_token = symbol        name = gecko_data[gecko_data['symbol'] == symbol]['name'].values[0]        gecko_id = gecko_data[gecko_data['symbol'] == symbol]['id'].values[0]                if symbol in chain_tokens:            wrapped_symbol = 'W' + symbol            address = token_addresses[wrapped_symbol]        else:            address = token_addresses[symbol]            llama_id = None                for key in token_categories.keys():            if symbol in (token_categories[key]):                category = key                        mini_dict = {'symbol': symbol, 'name': name, 'address': address, 'category': category, 'llama_id': llama_id, 'gecko_id': gecko_id}        BIG_DICT[key_token] = mini_dict                BIG_DICT[key_token]['prices'] = PMCV_dict[symbol]['prices']        BIG_DICT[key_token]['market_caps'] = PMCV_dict[symbol]['market_caps']        BIG_DICT[key_token]['volumes'] = PMCV_dict[symbol]['volumes']                try:            BIG_DICT[key_token]['TVL'] = TVL_dict[key_token]        except KeyError:            BIG_DICT[key_token]['TVL'] = None            final_dict = sort_dict_by_keys(BIG_DICT)    final_dataframe = dict_to_dataframe(final_dict, token_addresses)        return final_dataframe# Example# full_dataframe = get_full_data(token_industries = token_categories, start_date = '2021-08-31', end_date = today_date)